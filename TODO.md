# TODO

## Evaluation

- baselines
  - english electra-small finetune
  - seed model finetune
- more tasks
  - NER
  - POS
  - ???
- fixed number of fine-tuning epochs
- [superlim](https://spraakbanken.gu.se/projekt/superlim-en-svensk-testmangd-for-sprakmodeller/swedishglue-benchmark)
- send overlim data
- finetuning from electra

## Training

- optimization variables: yes or no?

## Other

- centralized vs federated optimizer
- What happens after warmup??
