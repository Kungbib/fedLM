# TODO

## Evaluation

- baselines
  - english electra-small finetune
  - seed model finetune
- more tasks
  - NER
  - POS
  - ???
- fixed number of fine-tuning epochs
- [superlim](https://spraakbanken.gu.se/projekt/superlim-en-svensk-testmangd-for-sprakmodeller/swedishglue-benchmark)

## Training

- 100 steps, 1000 steps, 2k, 5k
- after finding optimal step number
    - optimization variables: yes or no?

## Other

- centralized vs federated optimizer
- ~~validation before training~~ 
