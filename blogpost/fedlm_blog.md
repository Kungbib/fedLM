# A Federated Language Model

We trained a bilingual Swedish-Norwegian
[ELECTRA](https://github.com/google-research/electra) language model in
a federated setup, showcasing LM training when various corpora cannot be shared
directly.

## Introduction

Large transformer-based language models (LMs) have come to dominate the
state-of-the-art for many natural language processing (NLP) tasks.
These [models](https://huggingface.co/transformers/summary.html), such as BERT
and GPT, require both large amounts of compute as well as large amounts of
textual data.
Large tech companies that have been the driving force in the development of 
these large and steadily growing LMs, scrape the internet to gather huge text
corpora for many different genres.
These datasets however come with some problems.
Languages that are less common on the internet will be underrepresented and
the automatic classification of which language the text is actually in is not
necessarily very accurate either.
Due to the size of the data, manual checking is not feasible.
Including any type of text scraped from the internet without checking its 
content, will also include texts with undesirable views of racist,
sexist, or similar nature that can induce certain biases into the final model.
The [National Library of Sweden](https://www.kb.se/) (Kungliga Biblioteket --
KB) has access to vast amounts of digitized newspapers and other texts in
Swedish, that we used to train a state-of-the-art [Swedish
BERT](https://github.com/Kungbib/swedish-bert-models) language model.
In contrast to text scraped from the internet, our dataset is much more
controlled.
While it would be ideal to share the data directly, we unfortunately cannot due
to the copyright of the original owners of the individual texts.

In order to allow others to train new models with their own private and our
private data, _federated machine learning_ (FedML) can be used to train models
without directly sharing the data.
Such a setup would allow multiple national libraries and other maintainers of
private data, to collaborate in training multilingual LMs without having to 
sort out potential legal problems, as no data is shared.
We collaborate with [Scaleout](https://www.scaleoutsystems.com/) and use their
open-source FedML framework [FEDn](https://github.com/scaleoutsystems/fedn) to
train a Swedish-Norwegian ELECTRA language model.


## What is ELECTRA?

<img src="electra.jpg" width="200" height="auto" align="right" />

[ELECTRA](https://arxiv.org/pdf/2003.10555.pdf) is a transformer-based _masked
language model_ (MLM) similar to its predecessor BERT.
In contrast to classical LMs, now often referred to as _causal language models_
(CLMs), that are trained by predicting the next token in a sequence, an MLM is
trained by reconstructing the original sequence given a corrupted input
sequence.
In the original BERT model this is done by randomly masking out 15% of the
input:

> __Input:__ The `[MASK]` sat on the mat.
>
> __Output:__ The cat sat on the mat.

By learning to predict missing tokens, the model learns to imitate not only the
structure of language in form of fitting syntax, but also which words and
phrases have similar meaning by the contexts they have been used in the
dataset.

<!--- ![title](electra.jpg) --->

Given that only 15% of the input tokens are masked and thus used for training
the model, this approach is somewhat inefficient.
While the network structure of ELECTRA is essentially the same as BERT's, its
training objective promises to be more sample-efficient.
Instead of training to predict some masked out tokens, ELECTRA learns to
predict for each token whether it belongs to the original input sequence or
if it was generated by a secondary model.
This secondary model, the _generator_, is trained in tandem with the primary
model, the _discriminator_, quite similar to generative adversarial networks
(GANs).


> __Input:__ The dog sat on the mat.
>
> __Output:__  ‚úîÔ∏è ‚ùå ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è

With this new objective ELECTRA is able to outperform BERT, essentially 
applying the MLM objective to every input token.

## What is Federated Machine Learning?

Federated learning is a technique used, when a model needs to be trained on
multiple datasets that cannot be centralized.
There are two general usage scenarios for FedML cross-silo and cross-device.

__Cross-device__ is a scenario where there are too many small devices, such as
mobile or edge devices, that provide a constant stream of outputs.
In contrast to this, __cross-silo__ involves few, more powerful machines that
handle datasets that cannot be shared due to privacy concerns or legal
restrictions.

![title](HFedAvg.png)

In the FedML framework [FEDn](https://arxiv.org/pdf/2103.00148.pdf), we have
four different roles: i) controller,
ii) reducer, iii) combiner, and iv) client.
At the lowest level of this hierarchical structure, local models with local
data are trained on multiple geographically distributed _clients_.
These local models are then, after a certain number of training updates, sent
to one or more _combiners_ that coordinate the updates from their own subset of
_clients_.
These partial model updates are sent to the _reducer_, combining all model
updates into a single global model update.
Finally, the _controller's_ responsibility is to coordinate the overall
computation and to maintain the centralized models.

The update scheme used to combine the local models into one global model is
called [federated averaging (FedAvg)](https://arxiv.org/pdf/1602.05629.pdf),
one of the most widely used methods for FedML.
In each round the current version of the global model is distributed to the
clients that continue training using each their own data.
After one local round of training the distributed clients' model-weights are
sent back to the server that simply averages the weights, while taking the
number of local updates into account.

## Setup

With the future goal to train a large Scandinavian transformer-based language
model, we downscale the size of the model and data to be able to efficiently
test different hyper-parameter settings.
We choose to train a small ELECTRA model using publicly available data from the
[OSCAR corpus](https://oscar-corpus.com/) and Wikipedia, for Swedish, and
Norwegian _bokm√•l_ and _nynorsk_.
The Swedish corpus is with 27 GB, about five times larger than the 
Norwegian corpus.
This uneven distribution allows us to additionally investigate whether an LM
built on little data can benefit from a similar language's data.

Due to the rather small size of the small ELECTRA model, we were able to train
using standard workstation GPUs.
Our federated setup thus consists of three workstations plugged into the same
network, two of which serving as local clients, doing the majority of
computational work training the local model instances on GPU, and one
workstation taking care of collecting, averaging, and redistributing the
models.

Training large-scale transformer-based language models heavily relies on the
correct choice of hyper-parameters, for the model as well as the optimizer.
We follow the settings of the original small ELECTRA models in English, and
focus only on choosing the correct federated learning strategy.

In order to keep the communication overhead, that is the number of times the
local models are centralized, as low as possible, we would like to do as many
local model updates (i.e. gradient steps) as possible, without letting the 
local models diverge from one another too far.
Updating after for example 100 gradient steps will keep divergence to
a minimum and require fewer gradient steps in total to reach convergence, but
will, due its large communication overhead, need much longer in actual time to
reach convergence, than models communicating their updates after every 1000
local gradient steps.
On the other hand, taking too many local gradient steps will manage to do more
gradient steps in a shorter amount of time, but need many more updates and thus
time  to reach convergence.

With FedAvg we generally only consider model parameters, but large transformer
neural networks generally need more advanced optimization methods than the 
simple stochastic gradient descent.
In most cases the _Adam_ (Adaptive Moment Estimation) optimizer is used, which 
computes adaptive learning rates for each parameter, storing both the mean and
the variance of the gradients.
These additional parameters depend on the model parameters, meaning that they
should be averaged as well and redistributed to the clients.
This however increases the size of the data package that has to be sent by a
factor of three, which can be significant when larger models are trained that
"weigh" multiple gigabytes.
We test how the development of the loss is affected by keeping the optimizer
specific parameters local versus averaging them the same way as regular model
parameters.

## Results

To evaluate the impact of changing various hyper-parameters, we focus on the
development of the loss function during training.
While it seems easy to evaluate large language models, as one can simply use
the [GLUE](https://gluebenchmark.com/) or
[SuperGLUE](https://super.gluebenchmark.com/) benchmarks to get an overall
performance evaluation, there are many tricks one needs to apply to gain better
scores.
Even simply changing the random seed can [increase or decrease
performance](https://arxiv.org/pdf/2002.06305.pdf) by multiple points.

While we do not evaluate downstream model performance, we clearly see how the
training is affected.

### Number of Local Updates

In our first set of experiments we investigate how various local round lengths
affect the training progress.
We try four different local round lengths, with 100, 1000, 2000, and 5000 
gradient steps before recombining the models.

![title](../logs/round_lengths_steps.png)

While the loss decreases the most per steps taken when the model is updated as
often as possible (i.e. 100 steps), it takes far longer than in the other
setups to reach the same loss values.
Increasing the local round length to 5000 gradient steps allows us to do the
most gradient steps in the shortest amount of time, but results in the loss
not decreasing as quickly as with for example 1000 steps per round.

![title](../logs/round_lengths_time_long.png)

In this scenario we finally settle for 1000 steps per round, giving us the best
speed-performance trade-off.
With real-world models being much larger than the one used in our experiments,
it can be interesting to change the round length during training.
Longer round length in the beginning allows the model to see more data, while
shorter round lengths towards the end will help the model to converge.

### Local vs. Global Optimizer

Using a more advanced optimizer such as
[Adam](https://ruder.io/optimizing-gradient-descent/index.html#adam) is
necessary when training models with parameters now regularly surpassing
multiple billions.
This means unfortunately that the number of parameters that we need to
federate triples, which increases the communication overhead.
In order to test whether it is enough to only federate the model parameters
themselves while keeping the optimizer states local, we train our small
ELECTRA model with the additional Adam parameters retaining their local states,
and averaging them just as the regular model parameters.

![title](../figs/local_v_global_steps.png)

We can see that averaging the optimization-specific parameters allows the loss
to decrease further, without taking much more time.
While keeping the optimization parameters local increases the speed a little
bit (the green curve in the figure above is slightly longer), it is not enough
to counteract the decrease in learning.

![title](../figs/local_v_global_time.png)

These results show that keeping outdated optimization parameters to increase 
the overall speed is not desirable.
For larger models we might see a significant increase in speed, but it might
then be a better idea to change the optimization algorithm to regular
stochastic gradient descent, to avoid faulty inputs.
Similarly to dynamically changing the round lengths, adding a smarter
optimization algorithm towards the end can be a possibility.



## Continuation

This project has given us some promising first results towards training large
language models such as ELECTRA.
Using a federated black-box approach as implemented in FEDn, gives us the 
possibility to train models with other non-public data holders, but also gives
others the possibility to train their models with our data.

The models we trained are however only of one type and relatively small.
We are working on implementing an interface to the [ü§ó
Transformers](https://huggingface.co/transformers/) library, that will allow
users to train LMs from scratch in a federated fashion, but also fine-tune
these models using the same functionalities.
We hope that training models larger than our small ELECTRA, will give us more
insights into how long we should train locally and whether to change the 
optimization strategy.

With these pieces in place, we finally hope to train a large Scandinavian
language model that combines data sources that so far could not have been
combined.


<!---
### √ñverLim

#### Swedish


| Model                                     |   aggregated_g |   mnli |   mrpc |   qnli |    qqp |   rte   |    sst |   stsb |   wnli |
|-------------------------------------------|----------------|--------|--------|--------|--------|---------|--------|--------|--------|
| electra.no.small                          |         65.61% | 66.45% | 79.59% | 78.23% | 39.75% |  66.30% | 80.19% | 71.47% | 42.86% |
| electra.sv.small                          |         66.76% | 69.27% | 79.15% | 78.29% | 39.50% |  66.30% | 82.73% | 67.42% | 51.43% |
| electra.sv+no.small.centralized           |         66.95% | 69.25% | 78.53% | 78.80% | 39.20% |  67.39% | 82.27% | 71.56% | 48.57% |
| electra.sv+no.small.federated             |         67.59% | 73.64% | 74.86% | 80.62% | 39.83% |  71.01% | 85.98% | 76.21% | 38.57% |
| bert-base-swedish-cased                   |         72.63% | 81.55% | 81.52% | 87.59% | 43.12% |  77.17% | 92.35% | 84.89% | 32.86% |
| bert-base-multilingual-cased              |         72.75% | 77.48% | 81.93% | 86.04% | 41.88% |  75.72% | 90.85% | 82.42% | 45.71% |
| nb-bert-base                              |         73.36% | 80.44% | 81.99% | 86.82% | 41.77% |  78.62% | 90.96% | 83.38% | 42.86% |


| Model                                     |   aggregated_s |   boolq |     cb |   copa |   rte   |
|-------------------------------------------|----------------|---------|--------|--------|---------|
| electra.sv.small                          |         50.31% |  62.57% | 36.77% | 49.37% |  52.54% |
| nb-bert-base                              |         52.34% |  62.57% | 32.78% | 65.82% |  48.19% |
| electra.sv+no.small.federated             |         53.97% |  62.54% | 56.51% | 44.30% |  52.54% |
| electra.no.small                          |         54.86% |  62.57% | 56.60% | 43.04% |  57.25% |
| electra.sv+no.small.centralized           |         59.76% |  62.57% | 65.16% | 54.43% |  56.88% |
| bert-base-multilingual-cased              |         62.96% |  69.13% | 63.42% | 51.90% |  67.39% |
| bert-base-swedish-cased                   |         64.53% |  68.71% | 73.73% | 49.37% |  66.30% |

#### Norwegian

| Model                                     |   aggregated_g |   mnli |   mrpc |   qnli |    qqp |   rte   |    sst |   stsb |   wnli |
|-------------------------------------------|----------------|--------|--------|--------|--------|---------|--------|--------|--------|
| electra.sv.small                          |         62.30% | 64.36% | 76.84% | 76.21% | 38.92% |  60.51% | 76.83% | 60.44% | 44.29% |
| electra.sv+no.small.centralized           |         63.89% | 67.22% | 76.80% | 76.55% | 38.64% |  62.68% | 79.14% | 62.96% | 47.14% |
| electra.no.small                          |         64.53% | 68.16% | 76.65% | 77.30% | 38.95% |  63.41% | 79.61% | 65.03% | 47.14% |
| electra.sv+no.small.federated             |         66.47% | 70.08% | 76.91% | 79.43% | 39.31% |  63.04% | 81.69% | 68.45% | 52.86% |
| bert-base-swedish-cased                   |         67.26% | 73.14% | 76.70% | 81.65% | 41.69% |  68.84% | 86.44% | 73.93% | 35.71% |
| bert-base-multilingual-cased              |         68.99% | 73.57% | 80.61% | 83.37% | 41.83% |  65.94% | 87.49% | 76.26% | 42.86% |
| nb-bert-base                              |         71.20% | 76.80% | 77.97% | 83.96% | 42.27% |  69.93% | 88.99% | 76.85% | 52.86% |

|   Model                                   |   aggregated_s |   boolq |     cb |   copa |   rte   |
|-------------------------------------------|----------------|---------|--------|--------|---------|
| electra.sv+no.small.federated             |         50.32% |  62.86% | 37.59% | 49.37% |  51.45% |
| electra.sv.small                          |         51.16% |  62.57% | 33.83% | 50.63% |  57.61% |
| electra.no.small                          |         51.30% |  62.57% | 32.78% | 51.90% |  57.97% |
| electra.sv+no.small.centralized           |         55.04% |  62.57% | 56.59% | 45.57% |  55.43% |
| bert-base-multilingual-cased              |         56.32% |  67.22% | 47.12% | 49.37% |  61.59% |
| bert-base-swedish-cased                   |         59.29% |  67.28% | 60.02% | 49.37% |  60.51% |
| nb-bert-base                              |         64.41% |  66.04% | 68.87% | 60.76% |  61.96% |

--->
